import threading
import time
import string
import os
from .downloader import *
from .fetcher import *


class IssuuDownloadingManager:
    def __init__(self, number_of_threads, page_url, log_file, cache):
        assert number_of_threads >= 1
        self._cache = cache
        self._lock = threading.Lock()
        self._logging_callback = log_file.write
        self._estimated_file_count = self.estimate_number_of_documents_in_issuu_page(page_url)
        self._number_of_threads = number_of_threads
        self._page_url = page_url
        self._downloaded_so_far = {}
        self._threads = {}

    def estimate_number_of_documents_in_issuu_page(self, issuu_page_url):
        print(">> Estimating total workload")
        fetcher = IssuuFetcher(self._logging_callback)
        issuu_page_url_first_page = issuu_page_url + "/1"
        contents = fetcher.fetch_filter_and_extract_contents_from_issuu_page(issuu_page_url_first_page)
        contents_count = len(contents)
        elements = fetcher.fetch_filter_and_extract_pagination_data_from_issuu_page(issuu_page_url)
        max_page_index = 1
        for element in elements:
            for child in element.children:
                text = child.text.strip()
                if text.isdigit() and int(text) > max_page_index:
                    max_page_index = int(text)
        estimated_contents_count = (max_page_index - 1) * contents_count
        issuu_page_url_last_page = issuu_page_url + f"/{max_page_index}"
        last_page_contents = fetcher.fetch_filter_and_extract_contents_from_issuu_page(issuu_page_url_last_page)
        last_page_contents_count = len(last_page_contents)
        estimated_contents_count += last_page_contents_count
        print(">> Estimated total number of documents in issuu page: " + str(estimated_contents_count))
        return estimated_contents_count

    def _file_downloaded_callback(self, document_url, document_name):
        self._downloaded_so_far[document_name] = document_url
        percentage = int(len(self._downloaded_so_far) / self._estimated_file_count * 100)
        print(f"[{percentage}%]\t Downloaded {document_name}")

    def _file_skipped_callback(self, document_url, document_name):
        self._downloaded_so_far[document_name] = document_url
        percentage = int(len(self._downloaded_so_far) / self._estimated_file_count * 100)
        print(f"[{percentage}%]\t Cached {document_name}")

    def _format_file_name(self, non_formatted_filename):
        try:
            valid_chars = "-_.() %s%s" % (string.ascii_letters, string.digits)
            filename = ''.join(c for c in non_formatted_filename if c in valid_chars)
            self._logging_callback(f"Formatting filename")
            return filename
        except UnicodeError:
            string_timestamp = time.strftime("%Y_%m_%d_%H_%M_%S") + str(time.time())
            filename = f"AUTOGENERATED-FILENAME-{string_timestamp}_{non_formatted_filename}"
            self._logging_callback(f"Formatting filename")
            return filename

    def _get_output_file_path(self, output_dir, file_name):
        formatted_file_name = self._format_file_name(file_name)
        return os.path.join(output_dir, formatted_file_name) + ".pdf"

    def _download_some_issuu_documents_in_separate_thread(self, thread_index, download_path):
        page_index = thread_index
        fetched_contents = {}
        first_time = True
        while first_time or len(fetched_contents) > 0:
            first_time = False
            fetcher = IssuuFetcher(self._logging_callback)
            downloader = IssuuDownloader(self._logging_callback, self._file_downloaded_callback)
            page_url = f"{self._page_url}/{page_index}"
            fetched_contents = fetcher.fetch_filter_and_extract_contents_from_issuu_page(page_url)
            self._logging_callback(str(len(fetched_contents.items())))
            for document_name, document_url in fetched_contents.items():
                out_document_path = self._get_output_file_path(download_path, document_name)
                if self._cache.is_already_downloaded(document_url, out_document_path):
                    self._file_skipped_callback(document_url, document_name)
                    continue
                self._cache.mark_file_as_invalid(document_url)
                downloader.download_issuu_document_as_pdf(document_url, document_name, out_document_path)
                self._cache.mark_file_as_valid(document_url)
            page_index += self._number_of_threads
        with self._lock:
            self._threads.pop(thread_index)
        print(">> One thread finished execution")

    def download_every_issuu_document(self, download_path):
        print(f">> Launching multiple downloading threads: {self._number_of_threads}")
        with self._lock:
            for i in range(self._number_of_threads):
                thread_index = i + 1
                downloader_thread = threading.Thread(
                    target=self._download_some_issuu_documents_in_separate_thread,
                    args=(thread_index,download_path,),
                    daemon=True
                )
                downloader_thread.start()
                self._threads[thread_index] = downloader_thread
        while not len(self._threads) == 0:
            time.sleep(0.3)